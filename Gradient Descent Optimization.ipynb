{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimizers\n",
    "### Applying various algorithms that attempt to improve gradient descent\n",
    "Based on this overview: https://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.read_csv('Data/digits_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ddf.drop(['label'], axis=1).to_numpy()\n",
    "digit_y = ddf[['label']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_y = [[0]*10 for i in range(digit_y.shape[0])]\n",
    "\n",
    "for i in range(digit_y.shape[0]):\n",
    "    vector_y[i][digit_y[i][0]] = 1.0\n",
    "    \n",
    "y = np.array(vector_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate((X,y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(all_data)\n",
    "train = all_data[:10000]\n",
    "test = all_data[41000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, layers):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.l = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        for i in range(1, self.l):\n",
    "            self.weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i]))\n",
    "        \n",
    "    def train(self, training_data, time, learning_rate, batch_size, test_data=None):   \n",
    "        training_size = len(training_data)\n",
    "        self.rate = learning_rate\n",
    "        for i in range(time):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_x = [training_data[i:i+batch_size, :self.layers[0]] for i in range(0, training_size, batch_size)]\n",
    "            mini_y = [training_data[i:i+batch_size, self.layers[0]:] for i in range(0, training_size, batch_size)]\n",
    "            for x, y in zip(mini_x, mini_y):\n",
    "                self.feedforward(x)\n",
    "                self.backprop(y)\n",
    "            if test_data is not None:\n",
    "                print(f'epoch {i+1}: {round(self.evaluate(test_data)*100,2)}% accurate')\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        self.zs = []\n",
    "        self.activations = [x]\n",
    "        for i in range(self.l-1):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(z)\n",
    "            \n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def backprop(self, y):\n",
    "        rolling_delta = y - self.activations[-1]        \n",
    "        for i in range(-1, -(self.l), -1):\n",
    "            derivative = sigmoid_prime(self.zs[i])\n",
    "            \n",
    "            delta_b = rolling_delta * derivative\n",
    "            delta_w = np.dot(self.activations[i-1].T, rolling_delta * derivative)\n",
    "            \n",
    "            self.biases[i] += self.rate*np.sum(delta_b, axis=0)\n",
    "            self.weights[i] += self.rate*delta_w            \n",
    "            \n",
    "            rolling_delta = np.dot(rolling_delta * derivative, self.weights[i].T)\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        self.feedforward(test_data[:,:self.layers[0]])\n",
    "        digit_yhat = np.argmax(self.activations[-1], axis=1)\n",
    "        digit_y = np.argmax(test_data[:,self.layers[0]:], axis=1)\n",
    "        return sum([yhat == y for (yhat, y) in zip(digit_yhat, digit_y)]) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 11.1% accurate\n",
      "epoch 2: 10.1% accurate\n",
      "epoch 3: 13.4% accurate\n",
      "epoch 4: 13.0% accurate\n",
      "epoch 5: 13.1% accurate\n",
      "epoch 6: 13.8% accurate\n",
      "epoch 7: 14.8% accurate\n",
      "epoch 8: 16.0% accurate\n",
      "epoch 9: 17.5% accurate\n",
      "epoch 10: 18.5% accurate\n",
      "epoch 11: 20.9% accurate\n",
      "epoch 12: 22.0% accurate\n",
      "epoch 13: 23.4% accurate\n",
      "epoch 14: 25.4% accurate\n",
      "epoch 15: 26.9% accurate\n",
      "epoch 16: 28.3% accurate\n",
      "epoch 17: 30.5% accurate\n",
      "epoch 18: 32.7% accurate\n",
      "epoch 19: 34.8% accurate\n",
      "epoch 20: 37.5% accurate\n",
      "epoch 21: 39.0% accurate\n",
      "epoch 22: 41.5% accurate\n",
      "epoch 23: 42.6% accurate\n",
      "epoch 24: 44.7% accurate\n",
      "epoch 25: 46.3% accurate\n",
      "epoch 26: 48.4% accurate\n",
      "epoch 27: 49.6% accurate\n",
      "epoch 28: 51.4% accurate\n",
      "epoch 29: 52.7% accurate\n",
      "epoch 30: 53.8% accurate\n"
     ]
    }
   ],
   "source": [
    "no_opt = NeuralNet([784, 50, 30, 10])\n",
    "no_opt.train(train, 30, .001, 50, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "* Uses previous delta along with current delta\n",
    "* If one gradient keeps moving in the same direction that will change more than a gradient that is changing directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, layers):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.l = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        # adding self variables for the weights/biases deltas\n",
    "        # since we are tracking them over time now\n",
    "        self.weights_delta = []\n",
    "        self.biases_delta = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.l):\n",
    "            self.weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i]))\n",
    "            \n",
    "            # set inital deltas to be zero\n",
    "            self.weights_delta.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.biases_delta.append(np.zeros(layers[i]))\n",
    "        \n",
    "    def train(self, training_data, time, learning_rate, batch_size, moment, test_data=None):   \n",
    "        training_size = len(training_data)\n",
    "        self.rate = learning_rate\n",
    "        \n",
    "        # add a momentum term \n",
    "        self.momentum = moment\n",
    "        \n",
    "        for i in range(time):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_x = [training_data[i:i+batch_size, :self.layers[0]] for i in range(0, training_size, batch_size)]\n",
    "            mini_y = [training_data[i:i+batch_size, self.layers[0]:] for i in range(0, training_size, batch_size)]\n",
    "            for x, y in zip(mini_x, mini_y):\n",
    "                self.feedforward(x)\n",
    "                self.backprop(y)\n",
    "            if test_data is not None:\n",
    "                print(f'epoch {i+1}: {round(self.evaluate(test_data)*100,2)}% accurate')\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        self.zs = []\n",
    "        self.activations = [x]\n",
    "        for i in range(self.l-1):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(z)\n",
    "            \n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def backprop(self, y):        \n",
    "        rolling_delta = y - self.activations[-1]        \n",
    "        for i in range(-1, -(self.l), -1):\n",
    "            derivative = sigmoid_prime(self.zs[i])\n",
    "            \n",
    "            bias_grad = rolling_delta * derivative\n",
    "            weight_grad = np.dot(self.activations[i-1].T, rolling_delta * derivative)\n",
    "            \n",
    "            self.biases_delta[i] = self.momentum*self.biases_delta[i] + self.rate*np.sum(bias_grad, axis=0)\n",
    "            self.weights_delta[i] = self.momentum*self.weights_delta[i] + self.rate*weight_grad  \n",
    "            \n",
    "            self.biases[i] += self.biases_delta[i]\n",
    "            self.weights[i] += self.weights_delta[i]     \n",
    "            \n",
    "            rolling_delta = np.dot(rolling_delta * derivative, self.weights[i].T)\n",
    "\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        self.feedforward(test_data[:,:self.layers[0]])\n",
    "        digit_yhat = np.argmax(self.activations[-1], axis=1)\n",
    "        digit_y = np.argmax(test_data[:,self.layers[0]:], axis=1)\n",
    "        return sum([yhat == y for (yhat, y) in zip(digit_yhat, digit_y)]) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 27.9% accurate\n",
      "epoch 2: 36.4% accurate\n",
      "epoch 3: 42.9% accurate\n",
      "epoch 4: 45.7% accurate\n",
      "epoch 5: 47.3% accurate\n",
      "epoch 6: 49.1% accurate\n",
      "epoch 7: 50.1% accurate\n",
      "epoch 8: 51.6% accurate\n",
      "epoch 9: 53.5% accurate\n",
      "epoch 10: 55.8% accurate\n",
      "epoch 11: 57.7% accurate\n",
      "epoch 12: 60.1% accurate\n",
      "epoch 13: 63.1% accurate\n",
      "epoch 14: 65.2% accurate\n",
      "epoch 15: 65.7% accurate\n",
      "epoch 16: 67.7% accurate\n",
      "epoch 17: 69.1% accurate\n",
      "epoch 18: 70.4% accurate\n",
      "epoch 19: 75.8% accurate\n",
      "epoch 20: 78.4% accurate\n",
      "epoch 21: 78.8% accurate\n",
      "epoch 22: 79.5% accurate\n",
      "epoch 23: 80.5% accurate\n",
      "epoch 24: 80.7% accurate\n",
      "epoch 25: 81.3% accurate\n",
      "epoch 26: 82.2% accurate\n",
      "epoch 27: 82.8% accurate\n",
      "epoch 28: 83.0% accurate\n",
      "epoch 29: 84.3% accurate\n",
      "epoch 30: 84.7% accurate\n"
     ]
    }
   ],
   "source": [
    "moment = Momentum([784, 50, 30, 10])\n",
    "moment.train(train, 30, .001, 50, .9, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient (NAG)\n",
    "* builds on the momentum integration\n",
    "* we know our parameters are going to move by the momentum/previous delta term\n",
    "* so we apply the previous delta to the parameters before calculating the gradient\n",
    "* that way if the momentum would overshoot us we can correct for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAG:\n",
    "    def __init__(self, layers):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.l = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        # adding self variables for the weights/biases deltas\n",
    "        # since we are tracking them over time now\n",
    "        self.weights_delta = []\n",
    "        self.biases_delta = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.l):\n",
    "            self.weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i]))\n",
    "            \n",
    "            # set inital deltas to be zero\n",
    "            self.weights_delta.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.biases_delta.append(np.zeros(layers[i]))\n",
    "        \n",
    "    def train(self, training_data, time, learning_rate, batch_size, moment, test_data=None):   \n",
    "        training_size = len(training_data)\n",
    "        self.rate = learning_rate\n",
    "        \n",
    "        # add a momentum term \n",
    "        self.momentum = moment\n",
    "        \n",
    "        for i in range(time):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_x = [training_data[i:i+batch_size, :self.layers[0]] for i in range(0, training_size, batch_size)]\n",
    "            mini_y = [training_data[i:i+batch_size, self.layers[0]:] for i in range(0, training_size, batch_size)]\n",
    "            for x, y in zip(mini_x, mini_y):\n",
    "                self.feedforward(x, training=True)\n",
    "                self.backprop(y)\n",
    "            if test_data is not None:\n",
    "                print(f'epoch {i+1}: {round(self.evaluate(test_data)*100,2)}% accurate')\n",
    "            \n",
    "    def feedforward(self, x, training):\n",
    "        self.zs = []\n",
    "        self.activations = [x]\n",
    "        for i in range(self.l-1):\n",
    "            # use the previous delta to calculate the activations (unless using test data)\n",
    "            nag_weights = self.weights[i] + self.weights_delta[i] if training else self.weights[i]\n",
    "            nag_biases = self.biases[i] + self.biases_delta[i] if training else self.biases[i]\n",
    "            \n",
    "            z = np.dot(self.activations[-1], nag_weights) + nag_biases\n",
    "            self.zs.append(z)\n",
    "            \n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def backprop(self, y):        \n",
    "        rolling_delta = y - self.activations[-1]\n",
    "        for i in range(-1, -(self.l), -1):\n",
    "            derivative = sigmoid_prime(self.zs[i])\n",
    "            \n",
    "            bias_grad = rolling_delta * derivative\n",
    "            weight_grad = np.dot(self.activations[i-1].T, rolling_delta * derivative)\n",
    "            \n",
    "            self.biases_delta[i] = self.momentum*self.biases_delta[i] + self.rate*np.sum(bias_grad, axis=0)\n",
    "            self.weights_delta[i] = self.momentum*self.weights_delta[i] + self.rate*weight_grad  \n",
    "            \n",
    "            self.biases[i] += self.biases_delta[i]\n",
    "            self.weights[i] += self.weights_delta[i]\n",
    "            \n",
    "            rolling_delta = np.dot(rolling_delta * derivative, self.weights[i].T)\n",
    "\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        self.feedforward(test_data[:,:self.layers[0]], training=False)\n",
    "        digit_yhat = np.argmax(self.activations[-1], axis=1)\n",
    "        digit_y = np.argmax(test_data[:,self.layers[0]:], axis=1)\n",
    "        return sum([yhat == y for (yhat, y) in zip(digit_yhat, digit_y)]) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 20.4% accurate\n",
      "epoch 2: 34.4% accurate\n",
      "epoch 3: 44.9% accurate\n",
      "epoch 4: 52.3% accurate\n",
      "epoch 5: 56.4% accurate\n",
      "epoch 6: 61.0% accurate\n",
      "epoch 7: 65.7% accurate\n",
      "epoch 8: 68.9% accurate\n",
      "epoch 9: 71.1% accurate\n",
      "epoch 10: 72.3% accurate\n",
      "epoch 11: 73.7% accurate\n",
      "epoch 12: 74.8% accurate\n",
      "epoch 13: 76.5% accurate\n",
      "epoch 14: 77.7% accurate\n",
      "epoch 15: 78.0% accurate\n",
      "epoch 16: 79.4% accurate\n",
      "epoch 17: 80.1% accurate\n",
      "epoch 18: 80.3% accurate\n",
      "epoch 19: 81.2% accurate\n",
      "epoch 20: 81.5% accurate\n",
      "epoch 21: 82.0% accurate\n",
      "epoch 22: 82.8% accurate\n",
      "epoch 23: 83.0% accurate\n",
      "epoch 24: 83.7% accurate\n",
      "epoch 25: 83.6% accurate\n",
      "epoch 26: 84.4% accurate\n",
      "epoch 27: 84.3% accurate\n",
      "epoch 28: 85.1% accurate\n",
      "epoch 29: 84.6% accurate\n",
      "epoch 30: 85.2% accurate\n"
     ]
    }
   ],
   "source": [
    "nag = NAG([784, 50, 30, 10])\n",
    "nag.train(train, 30, .001, 50, .9, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "* adjusts learning rate based on parameters\n",
    "* smaller learning rate for frequently occurring\n",
    "* faster learning for infrequent\n",
    "* modifies general learning rate at each time step for each parameter based on past gradients\n",
    "  * weighti_t+1 = weighti_t - learning_rate/sqrt(Gi_t + epsilon) * gi_t\n",
    "  * where Gi_t is the sum of squares of past gradients\n",
    "  * epsilon is a small smoothing term to avoid dividing by zero (usually ~1e-8)\n",
    "* main weakness is the ever accumulating gradient in the denominator which causes learning rate to shrink\n",
    "* eventually stops learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, layers):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.l = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        # adding self variables for the weights/biases deltas\n",
    "        # since we are tracking them over time now\n",
    "        self.stacked_weights_delta = []\n",
    "        self.stacked_biases_delta = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.l):\n",
    "            self.weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i]))\n",
    "            \n",
    "            # set inital deltas to be zero\n",
    "            self.stacked_weights_delta.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.stacked_biases_delta.append(np.zeros(layers[i]))\n",
    "        \n",
    "    def train(self, training_data, time, learning_rate, batch_size, eps, test_data=None):   \n",
    "        training_size = len(training_data)\n",
    "        self.rate = learning_rate\n",
    "        self.eps = eps\n",
    "        \n",
    "        for i in range(time):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_x = [training_data[i:i+batch_size, :self.layers[0]] for i in range(0, training_size, batch_size)]\n",
    "            mini_y = [training_data[i:i+batch_size, self.layers[0]:] for i in range(0, training_size, batch_size)]\n",
    "            for x, y in zip(mini_x, mini_y):\n",
    "                self.feedforward(x)\n",
    "                self.backprop(y)\n",
    "            if test_data is not None:\n",
    "                print(f'epoch {i+1}: {round(self.evaluate(test_data)*100,2)}% accurate')\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        self.zs = []\n",
    "        self.activations = [x]\n",
    "        for i in range(self.l-1):            \n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(z)\n",
    "            \n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def backprop(self, y):        \n",
    "        rolling_delta = y - self.activations[-1]\n",
    "        for i in range(-1, -(self.l), -1):\n",
    "            derivative = sigmoid_prime(self.zs[i])\n",
    "            \n",
    "            bias_grad = np.sum(rolling_delta * derivative, axis=0)\n",
    "            weight_grad = np.dot(self.activations[i-1].T, rolling_delta * derivative)\n",
    "            \n",
    "            self.stacked_biases_delta[i] += bias_grad ** 2\n",
    "            self.stacked_weights_delta[i] += weight_grad ** 2\n",
    "            \n",
    "            self.biases[i] += bias_grad * self.rate / np.sqrt(self.stacked_biases_delta[i] + self.eps)\n",
    "            self.weights[i] += weight_grad * self.rate / np.sqrt(self.stacked_weights_delta[i] + self.eps)\n",
    "            \n",
    "            rolling_delta = np.dot(rolling_delta * derivative, self.weights[i].T)\n",
    "\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        self.feedforward(test_data[:,:self.layers[0]])\n",
    "        digit_yhat = np.argmax(self.activations[-1], axis=1)\n",
    "        digit_y = np.argmax(test_data[:,self.layers[0]:], axis=1)\n",
    "        return sum([yhat == y for (yhat, y) in zip(digit_yhat, digit_y)]) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 25.7% accurate\n",
      "epoch 2: 41.7% accurate\n",
      "epoch 3: 49.8% accurate\n",
      "epoch 4: 54.1% accurate\n",
      "epoch 5: 58.1% accurate\n",
      "epoch 6: 61.5% accurate\n",
      "epoch 7: 65.3% accurate\n",
      "epoch 8: 67.4% accurate\n",
      "epoch 9: 69.3% accurate\n",
      "epoch 10: 71.4% accurate\n",
      "epoch 11: 72.8% accurate\n",
      "epoch 12: 74.6% accurate\n",
      "epoch 13: 76.3% accurate\n",
      "epoch 14: 77.1% accurate\n",
      "epoch 15: 78.4% accurate\n",
      "epoch 16: 79.4% accurate\n",
      "epoch 17: 79.9% accurate\n",
      "epoch 18: 79.9% accurate\n",
      "epoch 19: 80.6% accurate\n",
      "epoch 20: 80.8% accurate\n",
      "epoch 21: 81.2% accurate\n",
      "epoch 22: 81.7% accurate\n",
      "epoch 23: 82.4% accurate\n",
      "epoch 24: 82.8% accurate\n",
      "epoch 25: 82.8% accurate\n",
      "epoch 26: 83.1% accurate\n",
      "epoch 27: 83.5% accurate\n",
      "epoch 28: 83.9% accurate\n",
      "epoch 29: 84.1% accurate\n",
      "epoch 30: 84.6% accurate\n"
     ]
    }
   ],
   "source": [
    "ada = Adagrad([784, 50, 30, 10])\n",
    "ada.train(training_data=train, time=30, learning_rate=.01, batch_size=50, eps=1e-8, test_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaDelta\n",
    "* adjusts the learning rate based on a window of past gradients\n",
    "* also incorporates time decayed average of past deltas\n",
    "* this makes it so that the parameter deltas have the same units as the parameters\n",
    "* weights_t+1 = weights_t - RMS(delta_t-1)/RMS(gradient_t) * gradient_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adadelta:\n",
    "    def __init__(self, layers):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.l = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        # adding self variables for the weights/biases deltas\n",
    "        # since we are tracking them over time now\n",
    "        self.rms_weight_delta = []\n",
    "        self.rms_biases_delta = []\n",
    "        self.rms_weight_grad = []\n",
    "        self.rms_biases_grad = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.l):\n",
    "            self.weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i]))\n",
    "            \n",
    "            # set inital deltas to be zero\n",
    "            self.rms_weight_delta.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.rms_biases_delta.append(np.zeros(layers[i]))\n",
    "            self.rms_weight_grad.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.rms_biases_grad.append(np.zeros(layers[i]))\n",
    "        \n",
    "    def train(self, training_data, time, decay, batch_size, eps, test_data=None):   \n",
    "        training_size = len(training_data)\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        \n",
    "        for i in range(time):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_x = [training_data[i:i+batch_size, :self.layers[0]] for i in range(0, training_size, batch_size)]\n",
    "            mini_y = [training_data[i:i+batch_size, self.layers[0]:] for i in range(0, training_size, batch_size)]\n",
    "            for x, y in zip(mini_x, mini_y):\n",
    "                self.feedforward(x)\n",
    "                self.backprop(y)\n",
    "            if test_data is not None:\n",
    "                print(f'epoch {i+1}: {round(self.evaluate(test_data)*100,2)}% accurate')\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        self.zs = []\n",
    "        self.activations = [x]\n",
    "        for i in range(self.l-1):            \n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(z)\n",
    "            \n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def backprop(self, y):        \n",
    "        rolling_delta = y - self.activations[-1]\n",
    "        for i in range(-1, -(self.l), -1):\n",
    "            derivative = sigmoid_prime(self.zs[i])\n",
    "            \n",
    "            biases_grad = np.sum(rolling_delta * derivative, axis=0)\n",
    "            weight_grad = np.dot(self.activations[i-1].T, rolling_delta * derivative)\n",
    "            \n",
    "            self.rms_biases_grad[i] = self.decay * self.rms_biases_grad[i] + (1-self.decay) * biases_grad**2\n",
    "            self.rms_weight_grad[i] = self.decay * self.rms_weight_grad[i] + (1-self.decay) * weight_grad**2\n",
    "            \n",
    "            biases_delta = biases_grad * np.sqrt(self.rms_biases_delta[i] + self.eps) / np.sqrt(self.rms_biases_grad[i] + self.eps)\n",
    "            weight_delta = weight_grad * np.sqrt(self.rms_weight_delta[i] + self.eps) / np.sqrt(self.rms_weight_grad[i] + self.eps)\n",
    "            \n",
    "            self.rms_biases_delta[i] = self.decay * self.rms_biases_delta[i] + (1-self.decay) * biases_delta**2\n",
    "            self.rms_weight_delta[i] = self.decay * self.rms_weight_delta[i] + (1-self.decay) * weight_delta**2\n",
    "            \n",
    "            self.biases[i] += biases_delta\n",
    "            self.weights[i] += weight_delta\n",
    "                       \n",
    "            rolling_delta = np.dot(rolling_delta * derivative, self.weights[i].T)\n",
    "\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        self.feedforward(test_data[:,:self.layers[0]])\n",
    "        digit_yhat = np.argmax(self.activations[-1], axis=1)\n",
    "        digit_y = np.argmax(test_data[:,self.layers[0]:], axis=1)\n",
    "        return sum([yhat == y for (yhat, y) in zip(digit_yhat, digit_y)]) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 11.1% accurate\n",
      "epoch 2: 17.8% accurate\n",
      "epoch 3: 32.5% accurate\n",
      "epoch 4: 43.0% accurate\n",
      "epoch 5: 49.8% accurate\n",
      "epoch 6: 54.6% accurate\n",
      "epoch 7: 58.5% accurate\n",
      "epoch 8: 61.5% accurate\n",
      "epoch 9: 65.1% accurate\n",
      "epoch 10: 66.6% accurate\n",
      "epoch 11: 67.4% accurate\n",
      "epoch 12: 69.2% accurate\n",
      "epoch 13: 70.8% accurate\n",
      "epoch 14: 72.5% accurate\n",
      "epoch 15: 72.9% accurate\n",
      "epoch 16: 73.9% accurate\n",
      "epoch 17: 75.3% accurate\n",
      "epoch 18: 76.2% accurate\n",
      "epoch 19: 76.6% accurate\n",
      "epoch 20: 77.4% accurate\n",
      "epoch 21: 77.7% accurate\n",
      "epoch 22: 78.4% accurate\n",
      "epoch 23: 78.7% accurate\n",
      "epoch 24: 80.3% accurate\n",
      "epoch 25: 80.5% accurate\n",
      "epoch 26: 80.5% accurate\n",
      "epoch 27: 81.1% accurate\n",
      "epoch 28: 81.6% accurate\n",
      "epoch 29: 81.8% accurate\n",
      "epoch 30: 81.8% accurate\n"
     ]
    }
   ],
   "source": [
    "ad = Adadelta([784, 50, 30, 10])\n",
    "ad.train(training_data=train, time=30, decay=.9, batch_size=50, eps=1e-8, test_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "* very similar to AdaDelta\n",
    "* developed around the same time but no association\n",
    "* uses a learning rate instead of incorporating the decayed accumulated deltas\n",
    "* weights_t+1 = weights_t - learning_rate/RMS(gradient_t) * gradient_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self, layers):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.l = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.rms_weight_grad = []\n",
    "        self.rms_biases_grad = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.l):\n",
    "            self.weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i]))\n",
    "            \n",
    "            # set inital deltas to be zero\n",
    "            self.rms_weight_grad.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.rms_biases_grad.append(np.zeros(layers[i]))\n",
    "        \n",
    "    def train(self, training_data, time, decay, learning_rate, batch_size, eps, test_data=None):   \n",
    "        training_size = len(training_data)\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.rate = learning_rate\n",
    "        \n",
    "        for i in range(time):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_x = [training_data[i:i+batch_size, :self.layers[0]] for i in range(0, training_size, batch_size)]\n",
    "            mini_y = [training_data[i:i+batch_size, self.layers[0]:] for i in range(0, training_size, batch_size)]\n",
    "            for x, y in zip(mini_x, mini_y):\n",
    "                self.feedforward(x)\n",
    "                self.backprop(y)\n",
    "            if test_data is not None:\n",
    "                print(f'epoch {i+1}: {round(self.evaluate(test_data)*100,2)}% accurate')\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        self.zs = []\n",
    "        self.activations = [x]\n",
    "        for i in range(self.l-1):            \n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(z)\n",
    "            \n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def backprop(self, y):        \n",
    "        rolling_delta = y - self.activations[-1]\n",
    "        for i in range(-1, -(self.l), -1):\n",
    "            derivative = sigmoid_prime(self.zs[i])\n",
    "            \n",
    "            biases_grad = np.sum(rolling_delta * derivative, axis=0)\n",
    "            weight_grad = np.dot(self.activations[i-1].T, rolling_delta * derivative)\n",
    "            \n",
    "            self.rms_biases_grad[i] = self.decay * self.rms_biases_grad[i] + (1-self.decay) * biases_grad**2\n",
    "            self.rms_weight_grad[i] = self.decay * self.rms_weight_grad[i] + (1-self.decay) * weight_grad**2\n",
    "            \n",
    "            biases_delta = biases_grad * self.rate / np.sqrt(self.rms_biases_grad[i] + self.eps)\n",
    "            weight_delta = weight_grad * self.rate / np.sqrt(self.rms_weight_grad[i] + self.eps)\n",
    "            \n",
    "            self.biases[i] += biases_delta\n",
    "            self.weights[i] += weight_delta\n",
    "                       \n",
    "            rolling_delta = np.dot(rolling_delta * derivative, self.weights[i].T)\n",
    "\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        self.feedforward(test_data[:,:self.layers[0]])\n",
    "        digit_yhat = np.argmax(self.activations[-1], axis=1)\n",
    "        digit_y = np.argmax(test_data[:,self.layers[0]:], axis=1)\n",
    "        return sum([yhat == y for (yhat, y) in zip(digit_yhat, digit_y)]) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 84.6% accurate\n",
      "epoch 2: 88.6% accurate\n",
      "epoch 3: 90.4% accurate\n",
      "epoch 4: 91.2% accurate\n",
      "epoch 5: 90.9% accurate\n",
      "epoch 6: 91.4% accurate\n",
      "epoch 7: 91.9% accurate\n",
      "epoch 8: 92.3% accurate\n",
      "epoch 9: 92.4% accurate\n",
      "epoch 10: 92.5% accurate\n",
      "epoch 11: 93.3% accurate\n",
      "epoch 12: 92.9% accurate\n",
      "epoch 13: 93.1% accurate\n",
      "epoch 14: 93.5% accurate\n",
      "epoch 15: 93.6% accurate\n",
      "epoch 16: 93.2% accurate\n",
      "epoch 17: 93.3% accurate\n",
      "epoch 18: 93.3% accurate\n",
      "epoch 19: 94.3% accurate\n",
      "epoch 20: 93.9% accurate\n",
      "epoch 21: 93.6% accurate\n",
      "epoch 22: 93.8% accurate\n",
      "epoch 23: 93.5% accurate\n",
      "epoch 24: 93.5% accurate\n",
      "epoch 25: 94.2% accurate\n",
      "epoch 26: 93.3% accurate\n",
      "epoch 27: 94.0% accurate\n",
      "epoch 28: 94.0% accurate\n",
      "epoch 29: 93.8% accurate\n",
      "epoch 30: 93.5% accurate\n"
     ]
    }
   ],
   "source": [
    "rms = RMSprop([784, 50, 30, 10])\n",
    "rms.train(training_data=train, time=30, decay=.9, learning_rate=.01, batch_size=50, eps=1e-8, test_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "* Incorporates adaptive learning rate (via decaying average of past squared gradients a la RMSprop/Adadelta) and momentum (via decaying average of past gradient)\n",
    "    * m_t = decay1 * m_t-1 + (1 - decay1) * gradient_t\n",
    "    * v_t = decay2 * v_t-1 + (1 - decay2) * gradient_t ^ 2\n",
    "* m_t is an estimate of the gradient mean and v_t is an estimate of the gradient variance\n",
    "* because both are set to 0 initial they are biased towards zero\n",
    "* compute bias-corrected estimates to counteract:\n",
    "  * m_t = m_t / (1 - decay1)\n",
    "  * v_t = v_t / (1 - decay2)\n",
    "* use corrected values to compute delta\n",
    "  * weights_t+1 = weights_t - learning_rate * m_t / (sqrt(v_t) + eps)\n",
    "* authors who came up with Adam suggested:\n",
    "  * decay1 = 0.9\n",
    "  * decay2 = 0.999\n",
    "  * eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, layers):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.l = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        # add variables for gradient mean and variance estimates\n",
    "        self.weight_m = []\n",
    "        self.biases_m = []\n",
    "        self.weight_v = []\n",
    "        self.biases_v = []\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.l):\n",
    "            self.weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i]))\n",
    "            \n",
    "            # set inital deltas to be zero\n",
    "            self.weight_m.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.biases_m.append(np.zeros(layers[i]))\n",
    "            self.weight_v.append(np.zeros((layers[i-1], layers[i])))          \n",
    "            self.biases_v.append(np.zeros(layers[i]))\n",
    "        \n",
    "    def train(self, training_data, time, mdecay, vdecay, learning_rate, batch_size, eps, test_data=None):   \n",
    "        training_size = len(training_data)\n",
    "        self.m_decay = mdecay\n",
    "        self.v_decay = vdecay\n",
    "        self.eps = eps\n",
    "        self.rate = learning_rate\n",
    "        \n",
    "        for i in range(time):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_x = [training_data[i:i+batch_size, :self.layers[0]] for i in range(0, training_size, batch_size)]\n",
    "            mini_y = [training_data[i:i+batch_size, self.layers[0]:] for i in range(0, training_size, batch_size)]\n",
    "            for x, y in zip(mini_x, mini_y):\n",
    "                self.feedforward(x)\n",
    "                self.backprop(y)\n",
    "            if test_data is not None:\n",
    "                print(f'epoch {i+1}: {round(self.evaluate(test_data)*100,2)}% accurate')\n",
    "            \n",
    "    def feedforward(self, x):\n",
    "        self.zs = []\n",
    "        self.activations = [x]\n",
    "        for i in range(self.l-1):            \n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.zs.append(z)\n",
    "            \n",
    "            a = sigmoid(z)\n",
    "            self.activations.append(a)\n",
    "            \n",
    "    def backprop(self, y):        \n",
    "        rolling_delta = y - self.activations[-1]\n",
    "        for i in range(-1, -(self.l), -1):\n",
    "            derivative = sigmoid_prime(self.zs[i])\n",
    "            \n",
    "            biases_grad = np.sum(rolling_delta * derivative, axis=0)\n",
    "            weight_grad = np.dot(self.activations[i-1].T, rolling_delta * derivative)\n",
    "            \n",
    "            # calculate exponentially decaying average of past gradients & squared gradients \n",
    "            self.biases_m[i] = self.m_decay * self.biases_m[i] + (1-self.m_decay) * biases_grad\n",
    "            self.weight_m[i] = self.m_decay * self.weight_m[i] + (1-self.m_decay) * weight_grad\n",
    "            self.biases_v[i] = self.v_decay * self.biases_v[i] + (1-self.v_decay) * biases_grad**2\n",
    "            self.weight_v[i] = self.v_decay * self.weight_v[i] + (1-self.v_decay) * weight_grad**2\n",
    "            \n",
    "            # correct for 0 bias\n",
    "            bias_mhat = self.biases_m[i]/(1-self.m_decay)\n",
    "            weight_mhat = self.weight_m[i]/(1-self.m_decay)\n",
    "            bias_vhat = self.biases_v[i]/(1-self.v_decay)\n",
    "            weight_vhat = self.weight_v[i]/(1-self.v_decay)\n",
    "            \n",
    "            self.biases[i] += bias_mhat * self.rate / (np.sqrt(bias_vhat) + self.eps)\n",
    "            self.weights[i] += weight_mhat * self.rate / (np.sqrt(weight_vhat) + self.eps)\n",
    "                       \n",
    "            rolling_delta = np.dot(rolling_delta * derivative, self.weights[i].T)\n",
    "\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        self.feedforward(test_data[:,:self.layers[0]])\n",
    "        digit_yhat = np.argmax(self.activations[-1], axis=1)\n",
    "        digit_y = np.argmax(test_data[:,self.layers[0]:], axis=1)\n",
    "        return sum([yhat == y for (yhat, y) in zip(digit_yhat, digit_y)]) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 62.9% accurate\n",
      "epoch 2: 65.2% accurate\n",
      "epoch 3: 71.5% accurate\n",
      "epoch 4: 78.5% accurate\n",
      "epoch 5: 79.4% accurate\n",
      "epoch 6: 80.4% accurate\n",
      "epoch 7: 82.6% accurate\n",
      "epoch 8: 91.4% accurate\n",
      "epoch 9: 91.0% accurate\n",
      "epoch 10: 91.9% accurate\n",
      "epoch 11: 91.7% accurate\n",
      "epoch 12: 92.1% accurate\n",
      "epoch 13: 91.8% accurate\n",
      "epoch 14: 92.3% accurate\n",
      "epoch 15: 91.9% accurate\n",
      "epoch 16: 92.3% accurate\n",
      "epoch 17: 91.4% accurate\n",
      "epoch 18: 92.2% accurate\n",
      "epoch 19: 92.1% accurate\n",
      "epoch 20: 92.3% accurate\n",
      "epoch 21: 92.1% accurate\n",
      "epoch 22: 92.0% accurate\n",
      "epoch 23: 92.4% accurate\n",
      "epoch 24: 92.7% accurate\n",
      "epoch 25: 92.5% accurate\n",
      "epoch 26: 92.8% accurate\n",
      "epoch 27: 92.4% accurate\n",
      "epoch 28: 93.3% accurate\n",
      "epoch 29: 92.4% accurate\n",
      "epoch 30: 93.1% accurate\n"
     ]
    }
   ],
   "source": [
    "adam = Adam([784, 50, 30, 10])\n",
    "adam.train(training_data=train, time=30, mdecay=.9, vdecay=.999, learning_rate=.01, batch_size=50, eps=1e-8, test_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaMax\n",
    "* updates Adam using infinity norm\n",
    "* from Adam: \n",
    " * v_t = decay2 * v_t-1 + (1 - decay2) * gradient_t^2\n",
    "* using infinity norm:\n",
    " * u_t = decay2^inf * v_t-1 + (1 - decay2^inf) * gradient_t^inf\n",
    " * u_t = max(decay2 * v_t-1, gradient_t)\n",
    "* weights_t+1 = weights_t - learning_rate/u_t * mhat_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nadam\n",
    "* Adam + NAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMSGrad\n",
    "* In some situations the exponential average of past squared gradients causes poor generalization\n",
    "* In settings where Adam converges to a suboptimal solution, it has been observed that some minibatches provide large and informative gradients, but as these minibatches only occur rarely, exponential averaging diminishes their influence, which leads to poor convergence. \n",
    "* AMSGrad fixes this issue by using the max past squared gradient instead of average\n",
    "* calculate v_t same as in Adam\n",
    "* then instead of the bias corrected vhat_t from Adam\n",
    "* vhat_t = max(vhat_t-1, v_t)\n",
    "* results in non-increasing step size\n",
    "* authors of this method saw improved performance (compared to Adam) on small datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
